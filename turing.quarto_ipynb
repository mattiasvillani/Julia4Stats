{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Probabilistic Programming in Turing.jl\"\n",
        "author: \"Mattias Villani\"\n",
        "editor: visual\n",
        "---\n",
        "\n",
        "#### IID normal model\n",
        "\n",
        "$$\n",
        "X_1,\\ldots,X_n \\vert \\mu, \\sigma^2 \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\n",
        "$$\n",
        "\n",
        "Prior\n",
        "\n",
        "$$\n",
        "\\sigma^2 \\sim \\chi^2(\\nu_0, \\sigma_0^2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu \\vert \\sigma^2 \\sim N\\Big(\\mu_0, \\frac{\\sigma^2}{\\kappa_0}\\Big)\n",
        "$$"
      ],
      "id": "d6aa069f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Turing, Plots, LaTeXStrings\n",
        "\n",
        "ScaledInverseChiSq(ν,τ²) = InverseGamma(ν/2,ν*τ²/2) # Scaled Inv-χ² distribution\n",
        "\n",
        "# Setting up the Turing model:\n",
        "@model function iidnormal(x, μ₀, κ₀, ν₀, σ²₀)\n",
        "    σ² ~ ScaledInverseChiSq(ν₀, σ²₀)\n",
        "    θ ~ Normal(μ₀, √(σ²/κ₀))  # prior\n",
        "    n = length(x)  # number of observations\n",
        "    for i in 1:n\n",
        "        x[i] ~ Normal(θ, √σ²) # model\n",
        "    end\n",
        "end\n",
        "\n",
        "# Set up the observed data\n",
        "x = [15.77,20.5,8.26,14.37,21.09]\n",
        "\n",
        "# Set up the prior\n",
        "μ₀ = 20; κ₀ = 1; ν₀ = 5; σ²₀ = 5^2\n",
        "\n",
        "# Settings of the Hamiltonian Monte Carlo (HMC) sampler.\n",
        "α = 0.8\n",
        "postdraws = sample(iidnormal(x, μ₀, κ₀, ν₀, σ²₀), NUTS(α), 10000, discard_initial = 1000)\n",
        "\n",
        "p1 = histogram(postdraws.value[:,2], yaxis = false, title = L\"\\mu\")\n",
        "p2 = histogram(sqrt.(postdraws.value[:,1]), yaxis = false, title = L\"\\sigma\")\n",
        "plot(p1, p2, layout = (1,2), size = (600,300))"
      ],
      "id": "bb102033",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Poisson regression**\n",
        "\n",
        "$$\n",
        "y_i \\vert \\boldsymbol{x}_i \\sim\\mathrm{Poisson}(\\lambda_i) \\\\\n",
        "$$\n",
        "\n",
        "with log link\n",
        "\n",
        "$$\n",
        "\\lambda_{i} =\\exp(\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "and multivariate normal prior\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\beta} \\sim N(\\boldsymbol{0}, \\tau^2 \\boldsymbol{I})\n",
        "$$\n",
        "\n",
        "![](figs/ebaydatatable.png)"
      ],
      "id": "937560d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Turing, CSV, Downloads, DataFrames, LinearAlgebra, LaTeXStrings, Plots\n",
        "\n",
        "# Reading and transforming the eBay data\n",
        "url = \"https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv\" \n",
        "df = CSV.read(Downloads.download(url), DataFrame)\n",
        "n = size(df,1)\n",
        "y = df[:,:NBidders]\n",
        "X = [ones(n,1) log.(df.BookVal) .- mean(log.(df.BookVal)) df.ReservePriceFrac .- mean(df.ReservePriceFrac) df.MinorBlem df.MajorBlem  df.NegFeedback df.PowerSeller df.IDSeller df.Sealed]\n",
        "\n",
        "varnames = [\"intercept\", \"logbook\", \"startprice\", \"minblemish\", \"majblemish\",    \n",
        "      \"negfeedback\", \"powerseller\", \"verified\", \"sealed\"]"
      ],
      "id": "74e76300",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Poisson regression with its prior is set up as Turing.jl using the @model macro:"
      ],
      "id": "fab9f622"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting up the poisson regression model\n",
        "@model function poissonReg(y, X, τ)\n",
        "    p = size(X,2)\n",
        "    β ~ filldist(Normal(0, τ), p)  # all βⱼ are iid Normal(0, τ)\n",
        "    λ = exp.(X*β)\n",
        "    n = length(y)  \n",
        "    for i in 1:n\n",
        "        y[i] ~ Poisson(λ[i]) \n",
        "    end\n",
        "end"
      ],
      "id": "37892d9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# HMC sampling from posterior\n",
        "p = size(X, 2)\n",
        "μ = zeros(p)    # Prior mean\n",
        "τ = 10          # Prior standard deviation Σ = τ²I\n",
        "α = 0.70        # target acceptance probability in NUTS sampler\n",
        "model = poissonReg(y, X, τ)\n",
        "chain = sample(model, NUTS(α), 10000, discard_initial = 1000, verbose = false)\n",
        "rateratio = exp.(chain.value) # exp(β) is the incidence rate ratio\n",
        "\n",
        "gr(grid = false)\n",
        "h = []\n",
        "for i = 1:p\n",
        "    ptmp = histogram(rateratio[:,i], nbins = 50, fillcolor = :steelblue, linecolor = nothing, \n",
        "        normalize = true, title = varnames[i], xlab = L\"\\exp(\\beta_{%$(i-1)})\", \n",
        "        yaxis = false, fillopacity = 0.5, label = \"\")    \n",
        "    push!(h, ptmp)\n",
        "end\n",
        "plot(h..., size = (600,600), legend = :right)"
      ],
      "id": "fffa39cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extensions to more complex models is easy. Negative binomial regression\n",
        "\n",
        "$$\n",
        "y_{i}\\vert\\boldsymbol{x}_{i} \\sim\\mathrm{NegBinomial}\\left(\\psi,p=\\frac{\\psi}{\\psi+\\lambda_{i}}\\right),\\quad\\lambda_{i}=\\exp(\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "``` julia\n",
        "# Negative binomial regression \n",
        "@model function negbinomialReg(y, X, τ, μ₀, σ₀)\n",
        "    p = size(X,2)\n",
        "    β ~ filldist(Normal(0, τ), p)  # all βⱼ are iid Normal(0, τ)\n",
        "    λ = exp.(X*β)\n",
        "    ψ ~ LogNormal(μ₀, σ₀)             # log of overdispersion parameter\n",
        "    n = length(y)  \n",
        "    for i in 1:n\n",
        "        y[i] ~ NegativeBinomial(ψ, ψ/(ψ + λ[i])) # mean is λ here, but var = λ(1 + λ/ψ) \n",
        "    end\n",
        "end\n",
        "\n",
        "μ₀ = 0   # Prior mean of log(ψ), where ψ is the overdispersion parameter\n",
        "σ₀ = 10  \n",
        "α = 0.70  # target acceptance probability in NUTS sampler\n",
        "model = negbinomialReg(y, X, τ, μ₀, σ₀)\n",
        "chain = sample(model, NUTS(α), 10000, discard_initial = 1000)\n",
        "rateratio = exp.(chain.value)\n",
        "```\n",
        "\n",
        "![](figs/ebay_negbin_overdispersion.png){width=\"500\"}\n",
        "\n",
        "#### Variational inference in Turing.jl\n",
        "\n",
        "``` julia\n",
        "# Variational inference assuming posterior is independent normals\n",
        "nSamples = 10\n",
        "nGradSteps = 1000\n",
        "approx_post = vi(model, ADVI(nSamples, nGradSteps)) # error when I try it now.\n",
        "approx_post.dist.m # mean of variational approximation\n",
        "approx_post.dist.σ # stdev of variational approximation\n",
        "βsample = rand(approx_post, 10000)\n",
        "```\n",
        "\n",
        "![](figs/ebay_post_meanratio_vs_advi.png){width=\"800\"}"
      ],
      "id": "94abd541"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.6",
      "path": "/home/mv/.local/share/jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}