[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia for Statistics - a tutorial",
    "section": "",
    "text": "Star\n\nThis tutorial was prepared as part of the COST action HiTEc for the Econometrics & Statistics (Ecosta) 2025 conference in Tokyo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAim\nThis tutorial introduces the Julia programming language and its use for statistical analysis.  The following topics will be covered:\n\nintroduction to the Julia programming language\nthe package manager and tooling\nmanaging data in Julia\nplots for statistics\nstatistical distributions\nlikelihood inference using numerical optimization\nworking with R and Python in Julia\nprobabilistic programming for statistical inference using Turing.jl\n\n\n\nInstructor\n\nMattias Villani Professor of Statistics Stockholm University\n\n\nBefore the tutorial\nYou may want to install Julia and VS code before the tutorial, to follow along in my interactive demos and also experiment yourself. I will not assume that you have installed Julia however, and you can attend the tutorial just to listen. See the first lecture for information on how to install Julia and (optionally, but recommended, VS Code). If you want to have the same packages and versions that I use in the tutorial, do this after you have installed Julia:\n\nDownload the Project.toml and Manifest.toml files and place them in your directory of choice.\nStart Julia by typing julia in the terminal.\nIn Julia, cd to your directory by typing cd(\"PathToYourDirectory\").\nIn Julia, type ] and press Enter to enter the package manager. The prompt should change to something with pkg&gt;.\nactivate the environment by typing activate . and Enter. (that is activate followed by space and a dot).\nInstantiate the environment with all dependencies by typing instantiate and Enter. This will takes some time since many packages are installed and precompiled.\nPress Backspace to exit the package manager.\n\n\n\nWorkshop plan, materials and schedule\n\nLecture 1 - The Julia programming language and tooling Time: 14.00-15.00  Reading: Getting started with Julia  Live demo: Basic Julia\nüçé leg stretcher\nLecture 2 - Working with data in Julia Time: 15.00-16.00  Reading: Read, managing and plotting data  Code: DataFrames.jl | Tidier.jl | Plots\n‚òï tea break\nLecture 3 - Statistics in Julia Time: 16.30-17.30  Reading: Distributions and Optimization | Interop with R and Python  Code: Distributions | Optimization | Working with R and Python\nüçì leg stretcher\nLecture 4 - Probabilistic programming using Turing.jl Time: 17.30-18.30  Reading: Probabilistic programming with Turing.jl\n\n\nMore material\nBooks, courses and podcasts etc about Julia A collection of Julia links Writing Julia packages More on packages and the package manager"
  },
  {
    "objectID": "workingwithdata.html",
    "href": "workingwithdata.html",
    "title": "Working with data",
    "section": "",
    "text": "CSV files can be read with the CSV.jl package:\n\nusing CSV, DataFrames\ntitanic = CSV.read(\"data/titanic.csv\", DataFrame);\nfirst(titanic, 3)\n\n3√ó8 DataFrame\n\n\n\nRow\nsurvived\npclass\nname\nsex\nage\nsibling/spouse\nparent/child\nfare\n\n\n\nInt64\nInt64\nString\nString7\nFloat64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n0\n3\nMr. Owen Harris Braund\nmale\n22.0\n1\n0\n7.25\n\n\n2\n1\n1\nMrs. John Bradley (Florence Briggs Thayer) Cumings\nfemale\n38.0\n1\n0\n71.2833\n\n\n3\n1\n3\nMiss. Laina Heikkinen\nfemale\n26.0\n0\n0\n7.925\n\n\n\n\n\n\n\nUse CSV.Rows or CSV.Chunks for reading only some observations at the time.\nWith StringEncodings.jl package, CSV can handle other character encodings.\nData can be read from an internet url with the help of the standard library Downloads.jl:\n\n\nusing CSV, DataFrames, Downloads\nurl = \"https://github.com/mattiasvillani/Julia4Stats/raw/main/data/titanic.csv\";\nhttp_response = Downloads.download(url);\ntitanic = CSV.read(http_response, DataFrame);\nfirst(titanic, 3)\n\n3√ó8 DataFrame\n\n\n\nRow\nsurvived\npclass\nname\nsex\nage\nsibling/spouse\nparent/child\nfare\n\n\n\nInt64\nInt64\nString\nString7\nFloat64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n0\n3\nMr. Owen Harris Braund\nmale\n22.0\n1\n0\n7.25\n\n\n2\n1\n1\nMrs. John Bradley (Florence Briggs Thayer) Cumings\nfemale\n38.0\n1\n0\n71.2833\n\n\n3\n1\n3\nMiss. Laina Heikkinen\nfemale\n26.0\n0\n0\n7.925"
  },
  {
    "objectID": "workingwithdata.html#read-and-writing-data-in-julia",
    "href": "workingwithdata.html#read-and-writing-data-in-julia",
    "title": "Working with data",
    "section": "",
    "text": "CSV files can be read with the CSV.jl package:\n\nusing CSV, DataFrames\ntitanic = CSV.read(\"data/titanic.csv\", DataFrame);\nfirst(titanic, 3)\n\n3√ó8 DataFrame\n\n\n\nRow\nsurvived\npclass\nname\nsex\nage\nsibling/spouse\nparent/child\nfare\n\n\n\nInt64\nInt64\nString\nString7\nFloat64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n0\n3\nMr. Owen Harris Braund\nmale\n22.0\n1\n0\n7.25\n\n\n2\n1\n1\nMrs. John Bradley (Florence Briggs Thayer) Cumings\nfemale\n38.0\n1\n0\n71.2833\n\n\n3\n1\n3\nMiss. Laina Heikkinen\nfemale\n26.0\n0\n0\n7.925\n\n\n\n\n\n\n\nUse CSV.Rows or CSV.Chunks for reading only some observations at the time.\nWith StringEncodings.jl package, CSV can handle other character encodings.\nData can be read from an internet url with the help of the standard library Downloads.jl:\n\n\nusing CSV, DataFrames, Downloads\nurl = \"https://github.com/mattiasvillani/Julia4Stats/raw/main/data/titanic.csv\";\nhttp_response = Downloads.download(url);\ntitanic = CSV.read(http_response, DataFrame);\nfirst(titanic, 3)\n\n3√ó8 DataFrame\n\n\n\nRow\nsurvived\npclass\nname\nsex\nage\nsibling/spouse\nparent/child\nfare\n\n\n\nInt64\nInt64\nString\nString7\nFloat64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n0\n3\nMr. Owen Harris Braund\nmale\n22.0\n1\n0\n7.25\n\n\n2\n1\n1\nMrs. John Bradley (Florence Briggs Thayer) Cumings\nfemale\n38.0\n1\n0\n71.2833\n\n\n3\n1\n3\nMiss. Laina Heikkinen\nfemale\n26.0\n0\n0\n7.925"
  },
  {
    "objectID": "workingwithdata.html#managing-data",
    "href": "workingwithdata.html#managing-data",
    "title": "Working with data",
    "section": "Managing data",
    "text": "Managing data\n DataFrames.jl is the original dataframe package in Julia.\n\nConstruct DataFrame (labeled matrix/table) using different approaches.\n\n\nusing DataFrames\ndf = DataFrame(\n    name = [\"Alice\", \"Bob\", \"Charlie\"],\n    age  = [25, 30, 35],\n    score = [88, 92, 95]\n)\n\n3√ó3 DataFrame\n\n\n\nRow\nname\nage\nscore\n\n\n\nString\nInt64\nInt64\n\n\n\n\n1\nAlice\n25\n88\n\n\n2\nBob\n30\n92\n\n\n3\nCharlie\n35\n95\n\n\n\n\n\n\n\nDataFrames.jl has some data wrangling functions like transform, filter, combine etc.\n\n\ndf2 = transform(df, :age =&gt; (x -&gt; 2x) =&gt; :age_doubled)\n\n3√ó4 DataFrame\n\n\n\nRow\nname\nage\nscore\nage_doubled\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nAlice\n25\n88\n50\n\n\n2\nBob\n30\n92\n60\n\n\n3\nCharlie\n35\n95\n70\n\n\n\n\n\n\nThe data frame can be modfied in place with the Julia convention to use exclamation mark (!) to denote a mutating function:\n\ntransform!(df, :age =&gt; (x -&gt; 2x) =&gt; :age_doubled)      # Modify df in place\n\n3√ó4 DataFrame\n\n\n\nRow\nname\nage\nscore\nage_doubled\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nAlice\n25\n88\n50\n\n\n2\nBob\n30\n92\n60\n\n\n3\nCharlie\n35\n95\n70\n\n\n\n\n\n\nWe can select a subset of the variables\n\ndf_selected = select(df, :name, :age_doubled)\n\n3√ó2 DataFrame\n\n\n\nRow\nname\nage_doubled\n\n\n\nString\nInt64\n\n\n\n\n1\nAlice\n50\n\n\n2\nBob\n60\n\n\n3\nCharlie\n70\n\n\n\n\n\n\nOr filter out observations:\n\ndf_filtered = filter(:age =&gt; &gt;(28), df)\n\n2√ó4 DataFrame\n\n\n\nRow\nname\nage\nscore\nage_doubled\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nBob\n30\n92\n60\n\n\n2\nCharlie\n35\n95\n70\n\n\n\n\n\n\n DataFramesMeta.jl uses Julia macros to @chain (pipe) together data manipulations, inspired by tidyverse in R.\n\nusing DataFramesMeta\ndf_extra = DataFrame(\n    name = [\"Alice\", \"Bob\", \"Diana\"],\n    city = [\"Stockholm\", \"Gothenburg\", \"Malm√∂\"]\n)\nresult = @chain df begin\n    @transform(:age_doubled = 2 .* :age)\n    @subset(_, :score .&gt; 90)       # pipes to first argument, here explicitly using _\n    @select(:name, :score, :age_doubled) \n    leftjoin(df_extra, on = :name)          # normal DataFrames function\nend\n\n2√ó4 DataFrame\n\n\n\nRow\nname\nscore\nage_doubled\ncity\n\n\n\nString\nInt64\nInt64\nString?\n\n\n\n\n1\nBob\n92\n60\nGothenburg\n\n\n2\nCharlie\n95\n70\nmissing\n\n\n\n\n\n\n TidierData.jl is a Julia re-implementation of the dplyr and tidyr packages from R.\n Tidier.jl is meta package, similar to the tidyverse package in R.\nusing CSV, DataFrames, Tidier\n\n# Read data from a URL using the standard library Downloads\nurl = \"https://github.com/mattiasvillani/Julia4Stats/raw/main/data/titanic.csv\";\nhttp_response = Downloads.download(url);\ntitanic = CSV.read(http_response, DataFrame)\n\n## TidierData.jl for data wrangling using @chain macro\ntitanic2 = @chain titanic begin\n    @mutate(survived = survived == 1)\n    @mutate(first_class = pclass == 1)\n    @filter(fare &gt; 10)\n    @select(name, survived, age, sex, first_class)\nend"
  },
  {
    "objectID": "workingwithdata.html#plotting-data",
    "href": "workingwithdata.html#plotting-data",
    "title": "Working with data",
    "section": "Plotting data",
    "text": "Plotting data\n Plots.jl - probably the most widely used\n Makie.jl - gaining popularity, advanced features\n TidierPlots.jl - Julia implementation of R‚Äôs ggplot2.\n Plotly.jl interface to the plot.ly library.\nand many many more."
  },
  {
    "objectID": "workingwithdata.html#plots",
    "href": "workingwithdata.html#plots",
    "title": "Working with data",
    "section": "Plots",
    "text": "Plots\n Plots.jl - a meta plotting package with many backends\n Makie.jl - gaining popularity, advanced features\n TidierPlots.jl - Julia implementation of R‚Äôs ggplot2.\n Plotly.jl interface to the plot.ly library.\nand many many more.\nHere is an example using Plots.jl to plot the mtcars data from Rdatasets.jl\n\nusing Plots, LaTeXStrings, RDatasets, GLM\n\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Make a scatter plot of Horsepower vs Miles per gallon\nscatter(mtcars.HP, mtcars.MPG, \n     xlabel = \"Horsepower\", ylabel = \"Miles per gallon\",\n     title = \"MPG vs Horsepower\", label = \"Data points\", \n     legend = :topright, color = :blue)\n\n# Fit a linear model using GLM.jl\nusing GLM\nlm_model = lm(@formula(MPG ~ HP), mtcars)\n\n# Add the fitted line to the plot, note the mutating plot! function\nplot!(mtcars.HP, predict(lm_model), \n     label = \"Fitted line\", color = :red, linewidth = 2)  \n\n# Add a LaTeX string to the title\nŒ≤hat = round.(coef(lm_model), digits = 3)\nplot!(title = L\"\\beta_0 = %$(Œ≤hat[1])\"* \" and \"* L\"\\beta_1 = %$(Œ≤hat[2])\")"
  },
  {
    "objectID": "workingwithdata.html#some-random-examples",
    "href": "workingwithdata.html#some-random-examples",
    "title": "Working with data",
    "section": "Some random examples",
    "text": "Some random examples"
  },
  {
    "objectID": "workingwithdata.html#makie.jl-example",
    "href": "workingwithdata.html#makie.jl-example",
    "title": "Working with data",
    "section": "Makie.jl example",
    "text": "Makie.jl example"
  },
  {
    "objectID": "workingwithdata.html#switching-backends",
    "href": "workingwithdata.html#switching-backends",
    "title": "Working with data",
    "section": "Switching backends",
    "text": "Switching backends\nLet us first plot a surface in Plots.jl:\n\nusing Plots, LaTeXStrings\n\n# Plot a surface with the gr backend\ngr()\nxs = range(-4, 4; length=150)\nys = range(-4, 4; length=150)\nf(x, y) = sin(x) * cos(y) * exp(-(x^2 + y^2)/8)\nPlots.surface(xs, ys, f; xlabel = L\"x\", ylabel = L\"y\", zlabel = L\"f(x,y)\", \n    legend=false, camera = (30, 60))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we switch backend to PlotlyJS to get an interactive plot where we can pan, zoom and rotate:\n\nimport PlotlyJS\nplotlyjs() # swithing to plotlyjs for interactive Plots\nPlots.surface(xs, ys, f; xlabel = \"x\", ylabel = \"y\", zlabel = \"f(x,y)\", \n    legend=false, camera = (30, 60))\n\n\nThe WebIO Jupyter extension was not detected. See the\n\n    WebIO Jupyter integration documentation\n\nfor more information."
  },
  {
    "objectID": "vscode.html",
    "href": "vscode.html",
    "title": "VS Code",
    "section": "",
    "text": "Access the command palette in VS code: Ctrl + Shift  + p\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrefix\n\nMode Name\n\nScope\n\nDescription\n\nExample\n\n\n\n\n&gt;\n\nCommand Mode\n\nGlobal\n\nRun VS Code commands\nThis is the default mode.\n\n&gt; settings\n\n\n@\n\nSymbol Navigation\n\nCurrent file\n\nNavigate to symbols, optionally sort or filter by type\n\n@ myFunc\n@: sorts objects by type, not by position in file like @ does.\n\n\n\n\nCycle between open files in the editor: Ctrl + Tab\nNavigate in the editor to previous cursor positions (even between files):\n\nGo back to previous cursor position: Alt + ‚Üê\nGo forward to next cursor position: Alt + ‚Üí\n\nToggle visibility of the terminal area: ctrl + j\nMaking VS code like RStudio‚Äôs window panes by placing the REPL in the editor area:\n\nctrl+shift+p to open command panel\nType settings and select the option Preferences: Open User Settings (not the JSON version)\nIn the Search settings box: type terminal defaultlocation to filter down to the setting that we want to change.\nIn Terminal &gt; Integrated: Default Location, select Editor in the dropdown menu.\nYou can now move the editor around freely by drag and drop of the window."
  },
  {
    "objectID": "vscode.html#useful-commands-and-navigation-in-vs-code",
    "href": "vscode.html#useful-commands-and-navigation-in-vs-code",
    "title": "VS Code",
    "section": "",
    "text": "Access the command palette in VS code: Ctrl + Shift  + p\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrefix\n\nMode Name\n\nScope\n\nDescription\n\nExample\n\n\n\n\n&gt;\n\nCommand Mode\n\nGlobal\n\nRun VS Code commands\nThis is the default mode.\n\n&gt; settings\n\n\n@\n\nSymbol Navigation\n\nCurrent file\n\nNavigate to symbols, optionally sort or filter by type\n\n@ myFunc\n@: sorts objects by type, not by position in file like @ does.\n\n\n\n\nCycle between open files in the editor: Ctrl + Tab\nNavigate in the editor to previous cursor positions (even between files):\n\nGo back to previous cursor position: Alt + ‚Üê\nGo forward to next cursor position: Alt + ‚Üí\n\nToggle visibility of the terminal area: ctrl + j\nMaking VS code like RStudio‚Äôs window panes by placing the REPL in the editor area:\n\nctrl+shift+p to open command panel\nType settings and select the option Preferences: Open User Settings (not the JSON version)\nIn the Search settings box: type terminal defaultlocation to filter down to the setting that we want to change.\nIn Terminal &gt; Integrated: Default Location, select Editor in the dropdown menu.\nYou can now move the editor around freely by drag and drop of the window."
  },
  {
    "objectID": "vscode.html#useful-commands-in-julia-for-vs-code",
    "href": "vscode.html#useful-commands-in-julia-for-vs-code",
    "title": "VS Code",
    "section": "Useful commands in Julia for VS Code",
    "text": "Useful commands in Julia for VS Code\n\nHere is the documentation for the Julia extension."
  },
  {
    "objectID": "vscode.html#useful-extensions",
    "href": "vscode.html#useful-extensions",
    "title": "VS Code",
    "section": "Useful extensions",
    "text": "Useful extensions\n\nJulia - Official support for the Julia language in VS code.\nR - Work with R in VS code.\nPython - Python language support.\nLaTeX workshop - LaTeX support in VS code.\nvscode-icons - brings nice symbols to folders and files.\nSubtle Match Brackets - nicer handling of matching brackets in the editor.\nLive Preview - live preview of HTML directly in VS code.\nMarkdown all in one - Markdown support in VS code.\nQuarto - Quarto support.\nCode Runner - run code in many different languages in VS code (not needed to run Julia code however since it has its own runner in Julia for VS code extension).\nExcel Viewer - Editors and previews for CSV files and Excel spreadsheets in Visual Studio Code.\nGit Lens - supercharge your Git experience in VS code.\nVS code Spotify - Play music in VS code."
  },
  {
    "objectID": "interop.html",
    "href": "interop.html",
    "title": "Julia + R + Python = True",
    "section": "",
    "text": "The RCall.jl package makes it possible to call R from Julia.\n\nAn R package or function may be lacking in Julia\nBenchmarking against methods in R packages.\n\nRCall.jl does:\n\nSends R commands to an R session in the background.\nMoves data from Julia to R, and R back to Julia.\nTransforms data structures in Julia to the ‚Äúright‚Äù data structure in R. Dictionary -&gt; list etc.\nEnter R mode by typing $ to get access to R console.\n\nTo send a command to R, use R\"julia_expression\"\n\n\nusing RCall         # Julia session must be set up, see RCall.jl docs\nR\"plot(rnorm(10))\";  # Evaluates in R\n\n\n\n\n\n\n\n\n\nVariables in Julia can be interpolated in R code using the $ -syntax\n\n\nn = 10\nR\"plot(rnorm($n))\";\n\n\n\n\n\n\n\n\n\nJulia variables can be sent to R with @rput .\nR variables can be sent back to Julia with @rget.\n\n\n# Just some regression data to play with\nn = 100\np = 3\nX = randn(n,p)\nŒ≤ = [1, 2, 0.5]\nœÉ = 0.3\ny  = X*Œ≤ + œÉ*randn(n)\n@rput y               # y now lives in the R session\n@rput X\nR\"modelFit &lt;- lm(y ~ X)\"\nR\"betaHat &lt;- modelFit$coef\"\nŒ≤hat = @rget betaHat  # Pull the R variable betaHat back to Julia\nŒ≤hat                  # lives in Julia now\n\n4-element Vector{Float64}:\n 0.043620495361893274\n 0.9772471884035804\n 2.014358371543607\n 0.592342891482621\n\n\n\nFor longer multiline code chunks, use the triple quotes:\n\nz = 2\n@rput z\nR\"\"\"\n    f &lt;- function(x, z) x + z\n    fvalue &lt;- f(1, $z)\n\"\"\"\nout = @rget fvalue # Pull the R variable out back to Julia\n\n3.0\n\n\nWrap R functions into Julia functions (works also for installed packages):\n\nfunction ARMAacf(ar, ma; lagmax, pacf = false) \n    R\"\"\"\n        acf_theo = ARMAacf(ar = $ar, ma = $ma, lag.max = $lagmax, pacf = $pacf)\n    \"\"\"\n    @rget acf_theo\n    return acf_theo\nend\n\nARMAacf([0.5, -0.2], [0.3]; lagmax = 5) # This is Julia function now\n\n6-element Vector{Float64}:\n  1.0\n  0.5646766169154229\n  0.08233830845771144\n -0.07176616915422887\n -0.052350746268656725\n -0.011822139303482589"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Julia resources",
    "section": "",
    "text": "The Julia Academy - great collection of learning resources."
  },
  {
    "objectID": "resources.html#cheat-sheets",
    "href": "resources.html#cheat-sheets",
    "title": "Julia resources",
    "section": "Cheat sheets",
    "text": "Cheat sheets\n\nhttps://cheatsheet.juliadocs.org/\n\n\nBooks\nHere is a list of books.\n\nJulia for Data Analysis\nHands-On Design Patterns and Best Practices\nJulia High Performance\nData Science using Julia\nJulia Quick Syntax Reference\n\n\n\nOnline courses\n\nJulia for Data Science\nLearn Julia in 4 hours (for beginners to programming)\nIntroduction to Computational thinking at MIT (using Julia throughout)\nAdvanced Scientific Computing (open source software development using Julia)\n\n\n\nPodcasts\n\nTalk Julia\nJulia dispatch\n\n\n\nVideos\nJulia in 100 seconds\nVS code in 100 seconds"
  },
  {
    "objectID": "building_packages.html",
    "href": "building_packages.html",
    "title": "Writing Julia packages",
    "section": "",
    "text": "It is quite easy to write packages in Julia, and many developers write small packages quite often.\nThere are well-developed packages for writing package documentation:\n\nDocumenter.jl is like roxygen2 for R. It turns documented code into a (html) manual.\nThe Literate.jl package is very useful for turning scripts into markdown files, often used for documented examples in a package. Literate.jl can return a markdown, a Jupyter or Quarto notebook, or a scrubbed Julia file without the markdown/comments.\n\nThe generate(‚ÄúExamplePackage‚Äù) command in the Package manager REPL initializes a new package.\nThe default package is very bare bone. Better to use a template from PkgTemplates.jl that sets up the author, license, Julia version, and Github actions for testing the code and building documentation automatically when pushed to Github. Here is an example template used to generate a package.\nusing PkgTemplates\nmyTemplate = Template(; \n    user=\"mattiasvillani\",\n    authors=[\"Mattias Villani\"],\n    julia=v\"1.11\",\n    plugins=[\n        License(; name=\"MIT\"),\n        Git(; manifest=true, ssh=true),\n        GitHubActions(; x86=true),\n        Codecov(),\n        Documenter{GitHubActions}(),\n        Develop(),\n    ],\n)\ngenerate(\"ExamplePackage\", myTemplate)\n\nThis generates the following folder structure for the package:\n\nThe folder structure contains:\n\nsrc is the main folder for the source code, with the package‚Äôs main module in the ExamplePackage.jl file.\ndocs is for the documentation, where the file make.jl contains what is needed to build the documentation. Run it to build the docs.\ntest is for unit tests.\nThe package also gets its own enviroment as seen by the generated Project.toml and Manifest.toml files.\nThe LICENSE and README.md files will determine the license and readme page once the package is pushed to Github.\nThe code also sets up a local git repository (in the hidden directory .git) with a remote address to a github repository (that you have to create on Github) named PackageName.jl. Note that the added .jl, which is standard for repo names on Github.\nUse the Revise.jl package so that changes in your package is picked up by Julia. Revise.jl is a no-brainer, everyone uses it. Most of us put a using Revise in the startup.jl filethat Julia reads before starting Julia (this file lives in ~/.julia/config/startup.jl on Linux, but you need to create the folder and file first time). Revise is however used automatically when developing with the Julia extension for VS Code."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Julia Links",
    "section": "",
    "text": "Install Julia\n Install VS Code\n Install Julia extension for VS Code | Guided Youtube video | Documentation\n Install and get started with Turing.jl | Turing tutorials"
  },
  {
    "objectID": "gettingstarted.html",
    "href": "gettingstarted.html",
    "title": "Getting started with Julia",
    "section": "",
    "text": "Julia is a high-level programming language for numerical computing.\nFirst version in 2012, Version 1.0 in 2018. Version 1.11 now.\nHigh-level programming with a syntax like Python.\nFast, almost like C/C++, if done right.\nJust-in-time (JIT) compiled. Code is compiled to machine code on the fly.\nNumerical linear algebra similar to Matlab.\nMacros inspired by Lisp.\nMultiple dispatch with sophisticated type system makes Julia highly composable."
  },
  {
    "objectID": "gettingstarted.html#what-is-julia",
    "href": "gettingstarted.html#what-is-julia",
    "title": "Getting started with Julia",
    "section": "",
    "text": "Julia is a high-level programming language for numerical computing.\nFirst version in 2012, Version 1.0 in 2018. Version 1.11 now.\nHigh-level programming with a syntax like Python.\nFast, almost like C/C++, if done right.\nJust-in-time (JIT) compiled. Code is compiled to machine code on the fly.\nNumerical linear algebra similar to Matlab.\nMacros inspired by Lisp.\nMultiple dispatch with sophisticated type system makes Julia highly composable."
  },
  {
    "objectID": "gettingstarted.html#julia-is-fast",
    "href": "gettingstarted.html#julia-is-fast",
    "title": "Getting started with Julia",
    "section": " Julia is fast",
    "text": "Julia is fast"
  },
  {
    "objectID": "gettingstarted.html#why-julia-for-statistics",
    "href": "gettingstarted.html#why-julia-for-statistics",
    "title": "Getting started with Julia",
    "section": " Why Julia (for Statistics)?",
    "text": "Why Julia (for Statistics)?\n\nSpeed. No need for a two language combo (R/Python and C/C++).\nGood packages for Statistics and Data Analysis.\nWonderful handling of statistical distributions.\nStrong linear algebra with beautiful syntax.\nParallel computing built in from the start.\nUnicode (e.g.¬†greek letters in code) makes clear algorithms.\nEasy to use R/Python/C/Fortran etc code from Julia."
  },
  {
    "objectID": "gettingstarted.html#installing-julia",
    "href": "gettingstarted.html#installing-julia",
    "title": "Getting started with Julia",
    "section": " Installing Julia",
    "text": "Installing Julia\nThe recommended way to install Julia is using a little tool called juliaup . Benefits of juliaup :\n\neasy install of Julia\neasy to switch between different Julia versions (see juliaup --help)\nnotifies when a new Julia version is available.\n\nInstructions on how to install juliaup are here. In short:\n\n Linux: run the following command in terminal:\ncurl -fsSL https://install.julialang.org | sh\nMac: run the following command in terminal:\ncurl -fsSL https://install.julialang.org | sh\n Windows, type this in the terminal to install from Windows store:\nwinget install --name Julia --id 9NJNWW8PVKMN -e -s msstore\n\n\n\n\n\n\n\nAvoid Linux repositories\n\n\n\nIt is not recommended to install Julia from repositories on Linux since the versions can be rather old.\n\n\n\n\n\n\n\n\nUninstall juliaup\n\n\n\nJuliaup can be uninstalled with the terminal command: juliaup self uninstall\n\n\nJulia can now be started by typing julia in the terminal (you may need to restart the shell/terminal so your system can find it). You should see something like this:"
  },
  {
    "objectID": "gettingstarted.html#the-julia-repl-console",
    "href": "gettingstarted.html#the-julia-repl-console",
    "title": "Getting started with Julia",
    "section": " The Julia REPL (console)",
    "text": "The Julia REPL (console)\n\nThe Julia REPL (Read-Evaluate-Print Loop) is where you can interactively execute code and view the output (like Console in R).\nWe will soon use VS Code as our main working environment (IDE, similar to RStudio), with the REPL as one component.\n\nThe Julia REPL is very good:\n\narrow-up and arrow-down goes back in command history.\nfiltered search in history. Start typing a word and then arrow-up will cycles though old commands that start with those letters.\nctrl+r gives more advanced reverse-search.\npast commands can be edited and re-used.\n\n\nThe REPL has several prompt modes, for example:\n\nJulian mode, which is the default julia&gt; seen above.\nHelp mode, activated by typing ? at the prompt. The prompt changes to help?&gt; and you can search for documentation about functions etc; see animated gif below. Backspace takes you back to Julian mode.\nPkg mode, activated by typing a right bracket ]. This opens the package manager where Julia packages are added and managed.\nShell mode, activated by typing a semicolon ;. The prompt changes to shell&gt; and you can use the usual shell commands for navigating directories, copying files etc. (On Windows, type powershell or cmd at shell&gt; to get access to the shell).\nSearch mode, activated by ctrl+r, as explained above.\nR mode. Later, when discuss how R and Julia can work together with the RCall.jl package, there will an R mode available by typing $ which opens an R prompt. More later.\n\n\nUseful keys bindings (here are many more):\n\nCtrl + C to interupt execution\nCtrl + L to clear the REPL\n\n\n\n\n\n\n\nExercise\n\n\n\n\nGet into help mode and read about the sin function.\nDrop into shell mode and change directory to somewhere else on your computer. Then go back to Julian mode (backspace) and check with pwd() that you indeed changed directory. Go back to shell mode and change the directory back to where you where before this exercise.\nGo into package mode and type status to see if you have any packages installed. (probably not if you have a fresh install)."
  },
  {
    "objectID": "gettingstarted.html#the-package-manager",
    "href": "gettingstarted.html#the-package-manager",
    "title": "Getting started with Julia",
    "section": " The package manager",
    "text": "The package manager\n\nSimilar to R, but a bit more extreme, most things in Julia are available in packages.\nPackages are typically named with a .jl ending, for example the Distributions.jl package contains many statistical distributions.\nJulia packages are hosted on GitHub and managed by the Julia General Registry.\n\nHere are some of the most common packages for Statistics:\n\nStatistics.jl (mean, variance, quantile etc)\nDistributions.jl (most standard statistical distributions, with rand, pdf, cdf etc)\nLinearAlgebra.jl\nDataFrames.jl (tabular data manipulation)\nTidier.jl (R‚Äôs tidyverse, Julia-style)\nPlots.jl (one of the most widely used plotting systems)\nOptim.jl (numerical optimization)\nGLM.jl (generalized linear models using R‚Äôs formula syntax)\nTuring.jl (probabilistic programming, similar to Stan)\nRCall.jl (using R code in Julia)\nDifferentiationInterface.jl (interface to Julia‚Äôs many autodiff libraries)\nRDatasets.jl (the R datasets that we all love, in Julia)\n\nThe package manager is most easily accessed by typing ] to get into Pkg mode. The prompt changes to (@v.11) pkg&gt; which means that you are in package mode in the default environment v.11 (if you are using Julia 1.11, more on environments later).\nSome useful commands in Pkg mode:\n\nstatus, shows the packages installed in the currently active environment\nadd PkgName (installs the PkgName.jl package, for example Distributions.jl)\nadd GitHubLink adds the package directly from Github, e.g.¬†add git@github.com:JuliaStats/Distributions.jl.git\nrm PkgName (removes the PkgName.jl package)\nupdate (checks registry and updates packages)\n\nWhen we write, for example, ] add Distributions we mean to type ] and then add Distributions on the Pkg prompt.\nOnce a package has been added it can be used from the Julian prompt (just backspace out from Pkg mode) with the using command: using PkgName . This loads the package in memory and its functions and other objects can now be used.\nThere are other ways to use a package, like import. The main difference is that\n\nusing exposes all exported function directly without needing to call then with the package name as prefix, e.g.¬†myfunc(3).\nimport does not expose exported function directly and requires a call to function to be prefixed with the package name, e.g.¬†PkgName.myfunc(3). This is closer to Python‚Äôs namespace convention, but namespaces are less essential in Julia due to multiple dispatch and Julias clever handling of function (method) name collisions.\nIt is also possible to use only specific functions from a package: using PkgName: myfunc, otherfunc.\nHere is comparison of package handling in R, Julia and Python.\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage\n\nRepository\n\nInstall packages\n\nUse packages\n\n\nR\n\nCRAN (Comprehensive R Archive Network)\n\ninstall.packages(\"pkgname\")\n\nlibrary(pkgname)\n\n\nJulia\n\nGeneral registry (hosted on GitHub)\n\n] add PkgName\n\nusing PkgName\nimport PkgName\nimport PkgName: myfunc\n\n\nPython\n\nPyPI (Python Package Index)\nor Conda\n\npip install pkgname\nconda install pkgname\n\nfrom pkgname import *\nimport pkgname as pk\nfrom pkgname import myfunc\n\n\n\n\n\nEnvironments\n\nPackages are by default add:ed to the default environment (e.g.¬†v1.11 if you use Julia 1.11). However, it is common to have several projects ongoing at the same time, and different projects needs different packages. Here is where environments are very useful.\nA Julia environment is basically a list of all the packages used in a project, potentially including detailed information about which version of the packages that are to be used in the project. An environment is created by going to a directory and typing activate . in the Pkg prompt. The dot means here (in the directory structure; alternatively, the dot can be replaced by a path where you want the project to live). The activate command creates two files:\n\nProject.toml - This is a list of the packages added to the project (so called direct dependencies), and their compatibility. Each package is identified by a unique ID. The link goes to the Project.toml used for this tutorial.\nManifest.toml - This contains the complete dependency structure of the added packages (since your added packages will often depend on other packages, so called indirect dependencies). The manifest.toml file can be used to instantiate all packages and their dependencies to the exact versions used in a particular project, so that code can be exactly reproduced with a frozen state of dependencies.\n\nWhen two projects use the same package at the same version, the content of this package is not duplicated.\n\n\n\n\n\n\n\nExercise\n\n\n\n\nAdd the Distributions.jl package in the package manager. Type status after install to see that it indeed was added.\nUse the Distributions.jl package by the using command from the Julian prompt.\nUse the package to define a normal distribution object with mean 2 and and standard deviation 3. Go into help mode and search for Normal to see how this is done. Call the new object dist.\nGenerate 10 random draws from this normal distribution using the rand function on the normal distribution object dist.\nEvaluate the pdf of this normal at x=1 using the pdf function on the normal distribution object dist."
  },
  {
    "objectID": "gettingstarted.html#running-programs",
    "href": "gettingstarted.html#running-programs",
    "title": "Getting started with Julia",
    "section": " Running programs",
    "text": "Running programs\nWe can write Julia code in any text editor and then execute it in the terminal with the julia command. Here is a small Julia example program (plotfunc.jl) that plots the square function using the Plots.jl package (which needs to add:ed first) and finally saves the graph to a pdf file.\n# adding Plots.jl package programatically (needed if it is not already installed)\nimport Pkg; Pkg.add(\"Plots\")\nusing Plots\n\n# Define the function that does the work \nfunction plotfunction(func, a, b)\n  xgrid = range(a, b, length = 100)\n  plt = plot(xgrid, func.(xgrid), color = :blue, xlabel = \"x\", ylabel = \"f(x)\")\n  return plt\nend\n\n# Use the function to produce the plot\nplt = plotfunction(x -&gt; x^2, -2, 2) # x -&gt; x^2 defines an anonymous function\n\n# save plot to file\nsavefig(plt, \"myplot.pdf\")\nWe can run this code in terminal with the command\njulia plotfunc.jl\n\n\n\n\n\n\nExercise\n\n\n\nCopy the code above to a text file name plotfunc.jl and run it using the command above. Check the pdf file."
  },
  {
    "objectID": "gettingstarted.html#vs-code---rstudio-for-julia",
    "href": "gettingstarted.html#vs-code---rstudio-for-julia",
    "title": "Getting started with Julia",
    "section": " VS code - Rstudio for Julia",
    "text": "VS code - Rstudio for Julia\n\nVS code\nThe most common way to work with Julia is using the VS code IDE (similar to RStudio, but more general). VS code is free and open source. It can be installed from here. There are many many extensions or plug-ins for VS code for doing mostly anything you want. Here is a 100 sec video about VS code. The official getting started guide is quite good.\nThe most important command in VS code is ctrl+shift+p which opens the Command palette. From there you can type things like settings to access all the settings, execute code etc.\n\n\nJulia in VS Code\nThe most important extension for Julia is the official Julia extension, which brings together the editor, Julia REPL, plot pane, debugging tools, and much more in one app. Once you open a .jl file in the editor, VS code will give you proper syntax highlighting. Here is 2+ minute getting started video. With the Julia extension installed, try ctrl+shift+p and type julia in the command palette to see some of the commands that you can use for Julia in VS code.\nThe official documention is here.\nHere are some useful commands, keyboard shortcuts and extensions in VS code.\n\nJulia code in VS code can be run in three basic ways:\n\nRun a file in a separate terminal: from command palette: Julia: Run File in New Process or from the Play button in the top right corner of the editor.\nDebug mode.\nRun interatively in the REPL inside VS code.\n\n\n\nshift + enter or shift + ctrl on a line in the editor starts the Julia REPL automatically. However, if you close the REPL, you have to use the command Julia: Restart REPL in the Command palette to start a new one.\nIf you prefer to not have the output inline in the editor, go to Julia extension settings and search for Execution: Result Type, and set it to REPL (the default is Both REPL and inline).\n\n\n\n\n\n\nExercise\n\n\n\nOpen the plotfunc.jl file from a previous exercise in VS Code and run it again using the command Julia: Run File in New Process\nNow, run each line in plotfunc.jl interactively in the VS code Julia REPL. The plot should appear in a plot pane in VS code."
  },
  {
    "objectID": "gettingstarted.html#notebooks",
    "href": "gettingstarted.html#notebooks",
    "title": "Getting started with Julia",
    "section": " Notebooks",
    "text": "Notebooks\nJulia can be used in all major notebook systems:\n Jupyter (The Ju in Jupyter stands for Julia). Install the IJulia package in Julia first.\n\n  Screenshot Jupyter notebook\n  \n\n Google Colab. Change to Julia in the Edit/Notebook settings menu.\n\n  Screenshot Google Colab notebook\n  \n\n Quarto. Install the R package JuliaCall in R first. Add engine: julia to YAML.\n\n  Screenshot Quarto notebook\n  \n\n Pluto.jl This is a reactive notebook system in Julia. Run from Julia or online.\n\n  Screenshot Pluto.jl notebook"
  },
  {
    "objectID": "gettingstarted.html#live-demo",
    "href": "gettingstarted.html#live-demo",
    "title": "Getting started with Julia",
    "section": "üíª Live demo",
    "text": "üíª Live demo\nLive demo of basic Julia roughly based on this script.\n\nJulia type hierarchy for Number type\n\nSource: cormullion?"
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "Statistics with Julia",
    "section": "",
    "text": "Julia has a fantastic unified system for statistical distributions, implemented mainly in the Distributions.jl package.\nSimilar to Python, but different from R, distributions are objects.\nWe can call functions on a dist = Normal() object, for example\n\npdf(dist, 2.0) computes the pdf at x=2.0\ncdf(dist, 2.0) computes the cdf at x=2.0\nrand(dist) generates random numbers from N(0,1)\nmean(dist) and var(dist) returns the mean and variance.\n\n\nusing Distributions\ndist = Normal(1,3) \npdf(dist, 0)\ncdf(dist, 0) \nquantile(dist, 0.5)\nrand(dist, 10) \n\n10-element Vector{Float64}:\n  3.85518075958924\n  0.3929149904692296\n  5.591344959436813\n -4.352515525717326\n -2.481811222302205\n -1.092084900621999\n -1.3110149146960683\n  0.32403752369845795\n  0.8145392548148882\n  0.904832550064212\n\n\nScale-locations families are generated by addition and multiplication\n\nusing Distributions\n\n1 + 2*TDist(3)\nTDist(Œº, œÉ, ŒΩ) = Œº + œÉ*TDist(ŒΩ)\nTDist(3)\ndist = TDist(1, 2, 3)\npdf(dist, 0)\n\n0.1565904555044143"
  },
  {
    "objectID": "distributions.html#statistical-distributions",
    "href": "distributions.html#statistical-distributions",
    "title": "Statistics with Julia",
    "section": "",
    "text": "Julia has a fantastic unified system for statistical distributions, implemented mainly in the Distributions.jl package.\nSimilar to Python, but different from R, distributions are objects.\nWe can call functions on a dist = Normal() object, for example\n\npdf(dist, 2.0) computes the pdf at x=2.0\ncdf(dist, 2.0) computes the cdf at x=2.0\nrand(dist) generates random numbers from N(0,1)\nmean(dist) and var(dist) returns the mean and variance.\n\n\nusing Distributions\ndist = Normal(1,3) \npdf(dist, 0)\ncdf(dist, 0) \nquantile(dist, 0.5)\nrand(dist, 10) \n\n10-element Vector{Float64}:\n  3.85518075958924\n  0.3929149904692296\n  5.591344959436813\n -4.352515525717326\n -2.481811222302205\n -1.092084900621999\n -1.3110149146960683\n  0.32403752369845795\n  0.8145392548148882\n  0.904832550064212\n\n\nScale-locations families are generated by addition and multiplication\n\nusing Distributions\n\n1 + 2*TDist(3)\nTDist(Œº, œÉ, ŒΩ) = Œº + œÉ*TDist(ŒΩ)\nTDist(3)\ndist = TDist(1, 2, 3)\npdf(dist, 0)\n\n0.1565904555044143"
  },
  {
    "objectID": "distributions.html#optimization-and-autodiff",
    "href": "distributions.html#optimization-and-autodiff",
    "title": "Statistics with Julia",
    "section": "Optimization and Autodiff",
    "text": "Optimization and Autodiff\nThe Optim.jl package is the main package for numerical function optimization."
  },
  {
    "objectID": "distributions.html#working-with-r-in-julia",
    "href": "distributions.html#working-with-r-in-julia",
    "title": "Statistics with Julia",
    "section": "Working with R in Julia",
    "text": "Working with R in Julia\nThe Rcall.jl package makes it possible to use R code, functions and packages from a Julia program."
  },
  {
    "objectID": "workingwithdata.html#some-random-examples-from-my-teaching",
    "href": "workingwithdata.html#some-random-examples-from-my-teaching",
    "title": "Working with data",
    "section": "Some random examples from my teaching",
    "text": "Some random examples from my teaching"
  },
  {
    "objectID": "dist_optim_R.html",
    "href": "dist_optim_R.html",
    "title": "Statistics with Julia",
    "section": "",
    "text": "Julia has a fantastic unified system for statistical distributions, implemented mainly in the Distributions.jl package.\nSimilar to Python, but different from R, distributions are objects.\n\nNormal(2,4) is a \\(N(2,4^2)\\) distribution object.\nPoisson(4) is a \\(\\mathrm{Poisson}(4)\\) distribution object.\n\n\nusing Distributions\ndist = Normal(1,3)\n\nNormal{Float64}(Œº=1.0, œÉ=3.0)\n\n\nWe can call functions on distribution objects.\nEvaluate the pdf at a point \\(x=0\\):\n\npdf(dist, 0)\n\n0.12579440923099772\n\n\nEvaluate the cumulative distribution function (cdf)\n\ncdf(dist, 0) \n\n0.36944134018176367\n\n\nCompute quantiles\n\nquantile(dist, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n -4.879891953620177\n  1.0\n  6.879891953620173\n\n\nGenerate random numbers\n\nrand(dist, 10) \n\n10-element Vector{Float64}:\n  2.133840009274354\n  3.7514097146843053\n -2.980432916964554\n  4.056663153113082\n -2.8383892115337233\n  1.334808378644338\n -4.0041927562895845\n  1.3005483572613565\n  5.558396602184795\n  3.6836291874390166\n\n\nLocation-Scale families are generated by addition and multiplication\n\n\nusing Distributions\n1 + 2*TDist(3)\n\nLocationScale{Int64, Continuous, TDist{Float64}}(\nŒº: 1\nœÉ: 2\nœÅ: TDist{Float64}(ŒΩ=3.0)\n)\n\n\nso one can easily define location-scale version\n\nTDist(Œº, œÉ, ŒΩ) = Œº + œÉ*TDist(ŒΩ)\ndist = TDist(1, 2, 3)\npdf(dist, 0)\n\n0.1565904555044143\n\n\n\nMixtures can be built up from component distributions with MixtureModel\n\n\n# Mixture of two normals\nmixdist = MixtureModel([Normal(0, 1), Normal(5, 3)], [0.3, 0.7])\npdf(mixdist, 1.0)\nquantile(mixdist, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n -1.628795787867508\n  3.303956200415996\n 10.408229272202913\n\n\n\n# Any number of mixture components, also discrete.\nmixdist = MixtureModel([Poisson(1), NegativeBinomial(2,0.3), Geometric(0.4)], [0.1, 0.2, 0.7])\npdf(mixdist, 4)\n\n0.059429831004881015\n\n\n\nTruncation\n\n\nusing Plots\n# Normal(0,1) truncated to [-1,1]\ndist = Truncated(Normal(0,1), -1, 1)\nxgrid = range(quantile(dist, 0.001), quantile(dist, 0.999), length = 500)\nplot(xgrid, pdf.(dist, xgrid), lw = 2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Gamma truncated from below at 1\ndist = Truncated(Gamma(2,2), 1, Inf)\nxgrid = range(0, quantile(dist, 0.999), length = 500)\nplot(xgrid, pdf.(dist, xgrid), lw = 2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensoring\n\n# Censored Weibull\ncdfval = cdf(Weibull(2,2),3)\nprintln(\"cdf at x=3 of Weibull without truncation is $(cdfval)\")\ndist = censored(Weibull(2,2), upper = 3)\nprintln(\"cdf at x=3 of Weibull with truncation at x=3 is $(cdf(dist,3))\")\nprintln(\"pdf at x=3 of Weibull with truncation at x=3 is $(pdf(dist,3))\")\n\ncdf at x=3 of Weibull without truncation is 0.8946007754381357\ncdf at x=3 of Weibull with truncation at x=3 is 1.0\npdf at x=3 of Weibull with truncation at x=3 is 0.10539922456186433\n\n\n\nWe can plot this by adding the extra point mass at the truncation point:\n\ndist = censored(Weibull(2,2), upper = 3)\nplot(0:0.01:4, pdf.(dist, 0:0.01:4), lw = 2)\n\n# Point mass\nplot!([3, 3], [0, pdf(dist,3)], lw=3, color=:indianred, legend=false)\nscatter!([3], [pdf(dist,3)], m = :circle, mc = :indianred, ms = 8, \n  lc = :indianred, lw=2)"
  },
  {
    "objectID": "dist_optim_R.html#statistical-distributions",
    "href": "dist_optim_R.html#statistical-distributions",
    "title": "Statistics with Julia",
    "section": "",
    "text": "Julia has a fantastic unified system for statistical distributions, implemented mainly in the Distributions.jl package.\nSimilar to Python, but different from R, distributions are objects.\n\nNormal(2,4) is a \\(N(2,4^2)\\) distribution object.\nPoisson(4) is a \\(\\mathrm{Poisson}(4)\\) distribution object.\n\n\nusing Distributions\ndist = Normal(1,3)\n\nNormal{Float64}(Œº=1.0, œÉ=3.0)\n\n\nWe can call functions on distribution objects.\nEvaluate the pdf at a point \\(x=0\\):\n\npdf(dist, 0)\n\n0.12579440923099772\n\n\nEvaluate the cumulative distribution function (cdf)\n\ncdf(dist, 0) \n\n0.36944134018176367\n\n\nCompute quantiles\n\nquantile(dist, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n -4.879891953620177\n  1.0\n  6.879891953620173\n\n\nGenerate random numbers\n\nrand(dist, 10) \n\n10-element Vector{Float64}:\n  2.133840009274354\n  3.7514097146843053\n -2.980432916964554\n  4.056663153113082\n -2.8383892115337233\n  1.334808378644338\n -4.0041927562895845\n  1.3005483572613565\n  5.558396602184795\n  3.6836291874390166\n\n\nLocation-Scale families are generated by addition and multiplication\n\n\nusing Distributions\n1 + 2*TDist(3)\n\nLocationScale{Int64, Continuous, TDist{Float64}}(\nŒº: 1\nœÉ: 2\nœÅ: TDist{Float64}(ŒΩ=3.0)\n)\n\n\nso one can easily define location-scale version\n\nTDist(Œº, œÉ, ŒΩ) = Œº + œÉ*TDist(ŒΩ)\ndist = TDist(1, 2, 3)\npdf(dist, 0)\n\n0.1565904555044143\n\n\n\nMixtures can be built up from component distributions with MixtureModel\n\n\n# Mixture of two normals\nmixdist = MixtureModel([Normal(0, 1), Normal(5, 3)], [0.3, 0.7])\npdf(mixdist, 1.0)\nquantile(mixdist, [0.025, 0.5, 0.975])\n\n3-element Vector{Float64}:\n -1.628795787867508\n  3.303956200415996\n 10.408229272202913\n\n\n\n# Any number of mixture components, also discrete.\nmixdist = MixtureModel([Poisson(1), NegativeBinomial(2,0.3), Geometric(0.4)], [0.1, 0.2, 0.7])\npdf(mixdist, 4)\n\n0.059429831004881015\n\n\n\nTruncation\n\n\nusing Plots\n# Normal(0,1) truncated to [-1,1]\ndist = Truncated(Normal(0,1), -1, 1)\nxgrid = range(quantile(dist, 0.001), quantile(dist, 0.999), length = 500)\nplot(xgrid, pdf.(dist, xgrid), lw = 2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Gamma truncated from below at 1\ndist = Truncated(Gamma(2,2), 1, Inf)\nxgrid = range(0, quantile(dist, 0.999), length = 500)\nplot(xgrid, pdf.(dist, xgrid), lw = 2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensoring\n\n# Censored Weibull\ncdfval = cdf(Weibull(2,2),3)\nprintln(\"cdf at x=3 of Weibull without truncation is $(cdfval)\")\ndist = censored(Weibull(2,2), upper = 3)\nprintln(\"cdf at x=3 of Weibull with truncation at x=3 is $(cdf(dist,3))\")\nprintln(\"pdf at x=3 of Weibull with truncation at x=3 is $(pdf(dist,3))\")\n\ncdf at x=3 of Weibull without truncation is 0.8946007754381357\ncdf at x=3 of Weibull with truncation at x=3 is 1.0\npdf at x=3 of Weibull with truncation at x=3 is 0.10539922456186433\n\n\n\nWe can plot this by adding the extra point mass at the truncation point:\n\ndist = censored(Weibull(2,2), upper = 3)\nplot(0:0.01:4, pdf.(dist, 0:0.01:4), lw = 2)\n\n# Point mass\nplot!([3, 3], [0, pdf(dist,3)], lw=3, color=:indianred, legend=false)\nscatter!([3], [pdf(dist,3)], m = :circle, mc = :indianred, ms = 8, \n  lc = :indianred, lw=2)"
  },
  {
    "objectID": "dist_optim_R.html#optimization-and-autodiff",
    "href": "dist_optim_R.html#optimization-and-autodiff",
    "title": "Statistics with Julia",
    "section": "Optimization and Autodiff",
    "text": "Optimization and Autodiff\nThe Optim.jl package is the main package for numerical function optimization.\nAs a simple example, consider finding the maximum likelihood estimate in Poisson regression with the BFGS optimizer.\nFirst, let us simulate some data from a Poisson regression model with an intercept and three covariates:\n\nusing Plots, Distributions, LinearAlgebra, Optim, ForwardDiff, Random, LaTeXStrings\n\n# Generate data from Poisson regression with Œ≤ = [1,-1,1,-1]\nn = 500\np = 4\nX = [ones(n,1) randn(n,p-1)]\nŒ≤ = 0.5*[1,-1,1,-1]\nŒª = exp.(X*Œ≤)\ny = rand.(Poisson.(Œª));\nscatter(X[:,2], y, title = \"Scatter of y against the first covariate\", \n  xlabel = L\"x_1\", ylabel = L\"y\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the ML estimate we need to define the log-likelihood function:\n\n# Setting up the log likelihood function for Poisson regression\nfunction poisreg_loglik(Œ≤, y, X)                        \n    return sum(logpdf.(Poisson.(exp.(X*Œ≤)), y))\nend\npoisreg_loglik(Œ≤, y, X) # Test drive the function to see that it works.\n\n-801.1549598275437\n\n\nWe can now use the maximize function to find the ML estimate:\n\nthe algorithm starts at the random initial value \\(\\beta_0\\).\nthe first argument in the function call is the log-likelihood function as an anonymous function of the single argument, the vector \\(\\beta\\).\nthe argument autodiff = :forward tells Julia to use automatic differentiation from the ForwardDiff.jl package to find the gradient vector, which is then used in the algorithm for finding the maximum.\nthe output from the maximize function is an object with information about the optimization result. The field maximizer contains the \\(\\beta\\) that maximizes the log-likelihood function.\n\n\n# Find the MLE of Œ≤ using Optim.jl\nŒ≤‚ÇÄ = randn(p) # Initial guess for the optimization\noptres = maximize(Œ≤ -&gt; poisreg_loglik(Œ≤, y, X), Œ≤‚ÇÄ, BFGS(), \n  autodiff = :forward)\nŒ≤mle = Optim.maximizer(optres)\n\n4-element Vector{Float64}:\n  0.4997494654733283\n -0.495306992311452\n  0.49073110848704\n -0.5209075829242628\n\n\nWe can approximate the standard errors of the ML estimator using the Hessian of the log-likelihood. This matrix of second partial derivatives can also be obtained from automatic differentiation. The diagonal elements of the negative inverse Hessian approximates the variance of the individual \\(\\hat\\beta_j\\) estimates:\n\n# Compute Hessian to get approximate standard errors\nH(Œ≤) = ForwardDiff.hessian(Œ≤ -&gt; poisreg_loglik(Œ≤, y, X), Œ≤)\nŒ©·µ¶ = Symmetric(-inv(H(Œ≤mle))) # This approximates the covariance matrix of MLE\ndiag(Œ©·µ¶) # Diagonal elements are the variances of the MLEs\nse = sqrt.(diag(Œ©·µ¶)) # Standard errors of the MLEs\nprintln(\"Œ≤mle: \\n\", round.(Œ≤mle, digits = 3))\nprintln(\"Standard errors: \\n\", round.(se, digits = 3))\n\nŒ≤mle: \n[0.5, -0.495, 0.491, -0.521]\nStandard errors: \n[0.038, 0.03, 0.024, 0.028]\n\n\nNote how the ForwardDiff.jl package computes the Hessian matrix as function, which can then be rapidly evaluated for any \\(\\beta\\) vector. Similarly, we can obtain the gradient as an explicit function and check that it is (numerically close to) the zero vector at the ML estimate:\n\n‚àÇloglik(Œ≤) = ForwardDiff.gradient(Œ≤ -&gt; poisreg_loglik(Œ≤, y, X), Œ≤)\n‚àÇloglik(Œ≤‚ÇÄ) # Gradient at the initial value, just because we can.\n‚àÇloglik(Œ≤mle) # Gradient at the MLE, should be close to zero\n\n4-element Vector{Float64}:\n  5.9190430334865596e-12\n -7.505773780280833e-12\n  2.4567015088905464e-12\n -1.0436096431476471e-14"
  },
  {
    "objectID": "dist_optim_R.html#working-with-r-in-julia",
    "href": "dist_optim_R.html#working-with-r-in-julia",
    "title": "Statistics with Julia",
    "section": "Working with R in Julia",
    "text": "Working with R in Julia\nThe Rcall.jl package makes it possible to use R code, functions and packages from a Julia program."
  },
  {
    "objectID": "ppl_turing.html",
    "href": "ppl_turing.html",
    "title": "Probabilistic Programming in Turing.jl",
    "section": "",
    "text": "IID normal model\n\\[\nX_1,\\ldots,X_n \\vert \\mu, \\sigma^2 \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\n\\]\nPrior\n\\[\n\\sigma^2 \\sim \\chi^2(\\nu_0, \\sigma_0^2)\n\\]\n\\[\n\\mu \\vert \\sigma^2 \\sim N\\Big(\\mu_0, \\frac{\\sigma^2}{\\kappa_0}\\Big)\n\\]\n\nusing Turing, Plots, LaTeXStrings\n\nScaledInverseChiSq(ŒΩ,œÑ¬≤) = InverseGamma(ŒΩ/2,ŒΩ*œÑ¬≤/2) # Scaled Inv-œá¬≤ distribution\n\n# Setting up the Turing model:\n@model function iidnormal(x, Œº‚ÇÄ, Œ∫‚ÇÄ, ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ)\n    œÉ¬≤ ~ ScaledInverseChiSq(ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ)\n    Œ∏ ~ Normal(Œº‚ÇÄ, ‚àö(œÉ¬≤/Œ∫‚ÇÄ))  # prior\n    n = length(x)  # number of observations\n    for i in 1:n\n        x[i] ~ Normal(Œ∏, ‚àöœÉ¬≤) # model\n    end\nend\n\n# Set up the observed data\nx = [15.77,20.5,8.26,14.37,21.09]\n\n# Set up the prior\nŒº‚ÇÄ = 20; Œ∫‚ÇÄ = 1; ŒΩ‚ÇÄ = 5; œÉ¬≤‚ÇÄ = 5^2\n\n# Settings of the Hamiltonian Monte Carlo (HMC) sampler.\nŒ± = 0.8\npostdraws = sample(iidnormal(x, Œº‚ÇÄ, Œ∫‚ÇÄ, ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ), NUTS(Œ±), 10000, discard_initial = 1000)\n\np1 = histogram(postdraws.value[:,2], yaxis = false, title = L\"\\mu\")\np2 = histogram(sqrt.(postdraws.value[:,1]), yaxis = false, title = L\"\\sigma\")\nplot(p1, p2, layout = (1,2), size = (600,300))\n\n\n‚îå Info: Found initial step size\n\n‚îî   œµ = 0.4\n\n\nSampling:  10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                    |  ETA: 0:00:01\n\nSampling:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              |  ETA: 0:00:01\n\nSampling:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        |  ETA: 0:00:01\n\nSampling:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  |  ETA: 0:00:00\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            |  ETA: 0:00:00\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      |  ETA: 0:00:00\n\nSampling:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã|  ETA: 0:00:00\n\nSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:00\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson regression\n\\[\ny_i \\vert \\boldsymbol{x}_i \\sim\\mathrm{Poisson}(\\lambda_i) \\\\\n\\]\nwith log link\n\\[\n\\lambda_{i} =\\exp(\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta})\n\\]\nand multivariate normal prior\n\\[\n\\boldsymbol{\\beta} \\sim N(\\boldsymbol{0}, \\tau^2 \\boldsymbol{I})\n\\]\n$$\n$$\n\n\nusing Turing, CSV, Downloads, DataFrames, LinearAlgebra, LaTeXStrings, Plots\n\n# Reading and transforming the eBay data\nurl = \"https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv\" \ndf = CSV.read(Downloads.download(url), DataFrame)\nn = size(df,1)\ny = df[:,:NBidders]\nX = [ones(n,1) log.(df.BookVal) .- mean(log.(df.BookVal)) df.ReservePriceFrac .- mean(df.ReservePriceFrac) df.MinorBlem df.MajorBlem  df.NegFeedback df.PowerSeller df.IDSeller df.Sealed]\n\nvarnames = [\"intercept\", \"logbook\", \"startprice\", \"minblemish\", \"majblemish\",    \n      \"negfeedback\", \"powerseller\", \"verified\", \"sealed\"]\n\n9-element Vector{String}:\n \"intercept\"\n \"logbook\"\n \"startprice\"\n \"minblemish\"\n \"majblemish\"\n \"negfeedback\"\n \"powerseller\"\n \"verified\"\n \"sealed\"\n\n\nThe Poisson regression with its prior is set up as Turing.jl using the @model macro:\n\n# Setting up the poisson regression model\n@model function poissonReg(y, X, œÑ)\n    p = size(X,2)\n    Œ≤ ~ filldist(Normal(0, œÑ), p)  # all Œ≤‚±º are iid Normal(0, œÑ)\n    Œª = exp.(X*Œ≤)\n    n = length(y)  \n    for i in 1:n\n        y[i] ~ Poisson(Œª[i]) \n    end\nend\n\npoissonReg (generic function with 2 methods)\n\n\n\n# HMC sampling from posterior\np = size(X, 2)\nŒº = zeros(p)    # Prior mean\nœÑ = 10          # Prior standard deviation Œ£ = œÑ¬≤I\nŒ± = 0.70        # target acceptance probability in NUTS sampler\nmodel = poissonReg(y, X, œÑ)\nchain = sample(model, NUTS(Œ±), 10000, discard_initial = 1000, verbose = false)\nrateratio = exp.(chain.value) # exp(Œ≤) is the incidence rate ratio\n\ngr(grid = false)\nh = []\nfor i = 1:p\n    ptmp = histogram(rateratio[:,i], nbins = 50, fillcolor = :indianred, linecolor = nothing, \n        normalize = true, title = varnames[i], xlab = L\"\\exp(\\beta_{%$(i-1)})\", \n        yaxis = false, fillopacity = 0.5)    \n    push!(h, ptmp)\nend\nplot(h..., size = (600,600), legend = :right)\n\n\n‚îå Info: Found initial step size\n\n‚îî   œµ = 4.656612873077393e-11\n\n\nSampling:   2%|‚ñã                                        |  ETA: 0:00:42\n\nSampling:   2%|‚ñà                                        |  ETA: 0:00:32\n\nSampling:   4%|‚ñà‚ñç                                       |  ETA: 0:00:26\n\nSampling:   4%|‚ñà‚ñâ                                       |  ETA: 0:00:24\n\nSampling:   6%|‚ñà‚ñà‚ñé                                      |  ETA: 0:00:21\n\nSampling:   6%|‚ñà‚ñà‚ñã                                      |  ETA: 0:00:20\n\nSampling:   8%|‚ñà‚ñà‚ñà‚ñè                                     |  ETA: 0:00:19\n\nSampling:   8%|‚ñà‚ñà‚ñà‚ñå                                     |  ETA: 0:00:18\n\nSampling:  10%|‚ñà‚ñà‚ñà‚ñâ                                     |  ETA: 0:00:17\n\nSampling:  10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                    |  ETA: 0:00:17\n\nSampling:  12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                    |  ETA: 0:00:16\n\nSampling:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   |  ETA: 0:00:16\n\nSampling:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   |  ETA: 0:00:16\n\nSampling:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   |  ETA: 0:00:16\n\nSampling:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  |  ETA: 0:00:15\n\nSampling:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  |  ETA: 0:00:15\n\nSampling:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 |  ETA: 0:00:15\n\nSampling:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 |  ETA: 0:00:15\n\nSampling:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 |  ETA: 0:00:14\n\nSampling:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                |  ETA: 0:00:14\n\nSampling:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                |  ETA: 0:00:14\n\nSampling:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               |  ETA: 0:00:14\n\nSampling:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               |  ETA: 0:00:13\n\nSampling:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               |  ETA: 0:00:13\n\nSampling:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              |  ETA: 0:00:13\n\nSampling:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              |  ETA: 0:00:13\n\nSampling:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             |  ETA: 0:00:13\n\nSampling:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             |  ETA: 0:00:13\n\nSampling:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             |  ETA: 0:00:13\n\nSampling:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            |  ETA: 0:00:12\n\nSampling:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            |  ETA: 0:00:12\n\nSampling:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           |  ETA: 0:00:12\n\nSampling:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           |  ETA: 0:00:12\n\nSampling:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           |  ETA: 0:00:12\n\nSampling:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          |  ETA: 0:00:12\n\nSampling:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          |  ETA: 0:00:11\n\nSampling:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         |  ETA: 0:00:11\n\nSampling:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         |  ETA: 0:00:11\n\nSampling:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         |  ETA: 0:00:11\n\nSampling:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        |  ETA: 0:00:11\n\nSampling:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        |  ETA: 0:00:10\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       |  ETA: 0:00:10\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       |  ETA: 0:00:10\n\nSampling:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       |  ETA: 0:00:10\n\nSampling:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      |  ETA: 0:00:10\n\nSampling:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      |  ETA: 0:00:10\n\nSampling:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     |  ETA: 0:00:10\n\nSampling:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     |  ETA: 0:00:09\n\nSampling:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     |  ETA: 0:00:09\n\nSampling:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    |  ETA: 0:00:09\n\nSampling:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    |  ETA: 0:00:09\n\nSampling:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    |  ETA: 0:00:09\n\nSampling:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   |  ETA: 0:00:09\n\nSampling:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   |  ETA: 0:00:09\n\nSampling:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   |  ETA: 0:00:08\n\nSampling:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  |  ETA: 0:00:08\n\nSampling:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  |  ETA: 0:00:08\n\nSampling:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 |  ETA: 0:00:08\n\nSampling:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 |  ETA: 0:00:08\n\nSampling:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 |  ETA: 0:00:08\n\nSampling:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                |  ETA: 0:00:07\n\nSampling:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                |  ETA: 0:00:07\n\nSampling:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                |  ETA: 0:00:07\n\nSampling:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               |  ETA: 0:00:07\n\nSampling:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               |  ETA: 0:00:07\n\nSampling:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               |  ETA: 0:00:07\n\nSampling:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              |  ETA: 0:00:06\n\nSampling:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              |  ETA: 0:00:06\n\nSampling:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             |  ETA: 0:00:06\n\nSampling:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             |  ETA: 0:00:06\n\nSampling:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            |  ETA: 0:00:06\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            |  ETA: 0:00:05\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            |  ETA: 0:00:05\n\nSampling:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           |  ETA: 0:00:05\n\nSampling:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           |  ETA: 0:00:05\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          |  ETA: 0:00:05\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          |  ETA: 0:00:05\n\nSampling:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          |  ETA: 0:00:04\n\nSampling:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         |  ETA: 0:00:04\n\nSampling:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         |  ETA: 0:00:04\n\nSampling:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        |  ETA: 0:00:04\n\nSampling:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        |  ETA: 0:00:04\n\nSampling:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        |  ETA: 0:00:03\n\nSampling:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       |  ETA: 0:00:03\n\nSampling:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       |  ETA: 0:00:03\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      |  ETA: 0:00:03\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      |  ETA: 0:00:03\n\nSampling:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      |  ETA: 0:00:03\n\nSampling:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     |  ETA: 0:00:03\n\nSampling:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     |  ETA: 0:00:02\n\nSampling:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    |  ETA: 0:00:02\n\nSampling:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    |  ETA: 0:00:02\n\nSampling:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    |  ETA: 0:00:02\n\nSampling:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   |  ETA: 0:00:02\n\nSampling:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   |  ETA: 0:00:01\n\nSampling:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  |  ETA: 0:00:01\n\nSampling:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  |  ETA: 0:00:01\n\nSampling:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  |  ETA: 0:00:01\n\nSampling:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç |  ETA: 0:00:01\n\nSampling:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä |  ETA: 0:00:01\n\nSampling:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè|  ETA: 0:00:00\n\nSampling:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã|  ETA: 0:00:00\n\nSampling:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä|  ETA: 0:00:00\n\nSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:17"
  },
  {
    "objectID": "turing.html",
    "href": "turing.html",
    "title": "Probabilistic Programming in Turing.jl",
    "section": "",
    "text": "IID normal model\n\\[\nX_1,\\ldots,X_n \\vert \\mu, \\sigma^2 \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\n\\]\nPrior\n\\[\n\\sigma^2 \\sim \\chi^2(\\nu_0, \\sigma_0^2)\n\\]\n\\[\n\\mu \\vert \\sigma^2 \\sim N\\Big(\\mu_0, \\frac{\\sigma^2}{\\kappa_0}\\Big)\n\\]\n\nusing Turing, Plots, LaTeXStrings\n\nScaledInverseChiSq(ŒΩ,œÑ¬≤) = InverseGamma(ŒΩ/2,ŒΩ*œÑ¬≤/2) # Scaled Inv-œá¬≤ distribution\n\n# Setting up the Turing model:\n@model function iidnormal(x, Œº‚ÇÄ, Œ∫‚ÇÄ, ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ)\n    œÉ¬≤ ~ ScaledInverseChiSq(ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ)\n    Œ∏ ~ Normal(Œº‚ÇÄ, ‚àö(œÉ¬≤/Œ∫‚ÇÄ))  # prior\n    n = length(x)  # number of observations\n    for i in 1:n\n        x[i] ~ Normal(Œ∏, ‚àöœÉ¬≤) # model\n    end\nend\n\n# Set up the observed data\nx = [15.77,20.5,8.26,14.37,21.09]\n\n# Set up the prior\nŒº‚ÇÄ = 20; Œ∫‚ÇÄ = 1; ŒΩ‚ÇÄ = 5; œÉ¬≤‚ÇÄ = 5^2\n\n# Settings of the Hamiltonian Monte Carlo (HMC) sampler.\nŒ± = 0.8\npostdraws = sample(iidnormal(x, Œº‚ÇÄ, Œ∫‚ÇÄ, ŒΩ‚ÇÄ, œÉ¬≤‚ÇÄ), NUTS(Œ±), 10000, discard_initial = 1000)\n\np1 = histogram(postdraws.value[:,2], yaxis = false, title = L\"\\mu\")\np2 = histogram(sqrt.(postdraws.value[:,1]), yaxis = false, title = L\"\\sigma\")\nplot(p1, p2, layout = (1,2), size = (600,300))\n\n\n‚îå Info: Found initial step size\n\n‚îî   œµ = 0.2\n\n\nSampling:  11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                    |  ETA: 0:00:01\n\nSampling:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                |  ETA: 0:00:01\n\nSampling:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            |  ETA: 0:00:01\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        |  ETA: 0:00:01\n\nSampling:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   |  ETA: 0:00:01\n\nSampling:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               |  ETA: 0:00:00\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          |  ETA: 0:00:00\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      |  ETA: 0:00:00\n\nSampling:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè |  ETA: 0:00:00\n\nSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:01\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson regression\n\\[\ny_i \\vert \\boldsymbol{x}_i \\sim\\mathrm{Poisson}(\\lambda_i) \\\\\n\\]\nwith log link\n\\[\n\\lambda_{i} =\\exp(\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta})\n\\]\nand multivariate normal prior\n\\[\n\\boldsymbol{\\beta} \\sim N(\\boldsymbol{0}, \\tau^2 \\boldsymbol{I})\n\\]\n\n\nusing Turing, CSV, Downloads, DataFrames, LinearAlgebra, LaTeXStrings, Plots\n\n# Reading and transforming the eBay data\nurl = \"https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv\" \ndf = CSV.read(Downloads.download(url), DataFrame)\nn = size(df,1)\ny = df[:,:NBidders]\nX = [ones(n,1) log.(df.BookVal) .- mean(log.(df.BookVal)) df.ReservePriceFrac .- mean(df.ReservePriceFrac) df.MinorBlem df.MajorBlem  df.NegFeedback df.PowerSeller df.IDSeller df.Sealed]\n\nvarnames = [\"intercept\", \"logbook\", \"startprice\", \"minblemish\", \"majblemish\",    \n      \"negfeedback\", \"powerseller\", \"verified\", \"sealed\"]\n\n9-element Vector{String}:\n \"intercept\"\n \"logbook\"\n \"startprice\"\n \"minblemish\"\n \"majblemish\"\n \"negfeedback\"\n \"powerseller\"\n \"verified\"\n \"sealed\"\n\n\nThe Poisson regression with its prior is set up as Turing.jl using the @model macro:\n\n# Setting up the poisson regression model\n@model function poissonReg(y, X, œÑ)\n    p = size(X,2)\n    Œ≤ ~ filldist(Normal(0, œÑ), p)  # all Œ≤‚±º are iid Normal(0, œÑ)\n    Œª = exp.(X*Œ≤)\n    n = length(y)  \n    for i in 1:n\n        y[i] ~ Poisson(Œª[i]) \n    end\nend\n\npoissonReg (generic function with 2 methods)\n\n\n\n# HMC sampling from posterior\np = size(X, 2)\nŒº = zeros(p)    # Prior mean\nœÑ = 10          # Prior standard deviation Œ£ = œÑ¬≤I\nŒ± = 0.70        # target acceptance probability in NUTS sampler\nmodel = poissonReg(y, X, œÑ)\nchain = sample(model, NUTS(Œ±), 10000, discard_initial = 1000, verbose = false)\nrateratio = exp.(chain.value) # exp(Œ≤) is the incidence rate ratio\n\ngr(grid = false)\nh = []\nfor i = 1:p\n    ptmp = histogram(rateratio[:,i], nbins = 50, fillcolor = :steelblue, linecolor = nothing, \n        normalize = true, title = varnames[i], xlab = L\"\\exp(\\beta_{%$(i-1)})\", \n        yaxis = false, fillopacity = 0.5, label = \"\")    \n    push!(h, ptmp)\nend\nplot(h..., size = (600,600), legend = :right)\n\n\n‚îå Info: Found initial step size\n\n‚îî   œµ = 2.9802322387695314e-9\n\n\nSampling:   2%|‚ñã                                        |  ETA: 0:00:24\n\nSampling:   2%|‚ñâ                                        |  ETA: 0:00:26\n\nSampling:   2%|‚ñà                                        |  ETA: 0:00:25\n\nSampling:   3%|‚ñà‚ñé                                       |  ETA: 0:00:25\n\nSampling:   4%|‚ñà‚ñç                                       |  ETA: 0:00:25\n\nSampling:   4%|‚ñà‚ñâ                                       |  ETA: 0:00:24\n\nSampling:   6%|‚ñà‚ñà‚ñé                                      |  ETA: 0:00:22\n\nSampling:   6%|‚ñà‚ñà‚ñã                                      |  ETA: 0:00:21\n\nSampling:   8%|‚ñà‚ñà‚ñà‚ñè                                     |  ETA: 0:00:20\n\nSampling:   8%|‚ñà‚ñà‚ñà‚ñå                                     |  ETA: 0:00:19\n\nSampling:   9%|‚ñà‚ñà‚ñà‚ñä                                     |  ETA: 0:00:19\n\nSampling:  10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                    |  ETA: 0:00:19\n\nSampling:  10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                    |  ETA: 0:00:19\n\nSampling:  11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                    |  ETA: 0:00:19\n\nSampling:  12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                    |  ETA: 0:00:19\n\nSampling:  12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                    |  ETA: 0:00:19\n\nSampling:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   |  ETA: 0:00:19\n\nSampling:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   |  ETA: 0:00:19\n\nSampling:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   |  ETA: 0:00:19\n\nSampling:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   |  ETA: 0:00:19\n\nSampling:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   |  ETA: 0:00:19\n\nSampling:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  |  ETA: 0:00:18\n\nSampling:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  |  ETA: 0:00:18\n\nSampling:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  |  ETA: 0:00:18\n\nSampling:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  |  ETA: 0:00:18\n\nSampling:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  |  ETA: 0:00:18\n\nSampling:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 |  ETA: 0:00:18\n\nSampling:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 |  ETA: 0:00:18\n\nSampling:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 |  ETA: 0:00:18\n\nSampling:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 |  ETA: 0:00:17\n\nSampling:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                |  ETA: 0:00:17\n\nSampling:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                |  ETA: 0:00:17\n\nSampling:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                |  ETA: 0:00:17\n\nSampling:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                |  ETA: 0:00:17\n\nSampling:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               |  ETA: 0:00:17\n\nSampling:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               |  ETA: 0:00:16\n\nSampling:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               |  ETA: 0:00:16\n\nSampling:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              |  ETA: 0:00:16\n\nSampling:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              |  ETA: 0:00:16\n\nSampling:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              |  ETA: 0:00:16\n\nSampling:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             |  ETA: 0:00:16\n\nSampling:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             |  ETA: 0:00:16\n\nSampling:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             |  ETA: 0:00:15\n\nSampling:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            |  ETA: 0:00:15\n\nSampling:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            |  ETA: 0:00:15\n\nSampling:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            |  ETA: 0:00:15\n\nSampling:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            |  ETA: 0:00:15\n\nSampling:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           |  ETA: 0:00:14\n\nSampling:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           |  ETA: 0:00:14\n\nSampling:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           |  ETA: 0:00:14\n\nSampling:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          |  ETA: 0:00:14\n\nSampling:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          |  ETA: 0:00:14\n\nSampling:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          |  ETA: 0:00:13\n\nSampling:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         |  ETA: 0:00:13\n\nSampling:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         |  ETA: 0:00:13\n\nSampling:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         |  ETA: 0:00:13\n\nSampling:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        |  ETA: 0:00:13\n\nSampling:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        |  ETA: 0:00:13\n\nSampling:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        |  ETA: 0:00:13\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        |  ETA: 0:00:12\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       |  ETA: 0:00:12\n\nSampling:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       |  ETA: 0:00:12\n\nSampling:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       |  ETA: 0:00:12\n\nSampling:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       |  ETA: 0:00:12\n\nSampling:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      |  ETA: 0:00:12\n\nSampling:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      |  ETA: 0:00:12\n\nSampling:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      |  ETA: 0:00:12\n\nSampling:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      |  ETA: 0:00:12\n\nSampling:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     |  ETA: 0:00:11\n\nSampling:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     |  ETA: 0:00:11\n\nSampling:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     |  ETA: 0:00:11\n\nSampling:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    |  ETA: 0:00:11\n\nSampling:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    |  ETA: 0:00:11\n\nSampling:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    |  ETA: 0:00:10\n\nSampling:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   |  ETA: 0:00:10\n\nSampling:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   |  ETA: 0:00:10\n\nSampling:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   |  ETA: 0:00:10\n\nSampling:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   |  ETA: 0:00:10\n\nSampling:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  |  ETA: 0:00:10\n\nSampling:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  |  ETA: 0:00:10\n\nSampling:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  |  ETA: 0:00:09\n\nSampling:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  |  ETA: 0:00:09\n\nSampling:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 |  ETA: 0:00:09\n\nSampling:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 |  ETA: 0:00:09\n\nSampling:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 |  ETA: 0:00:09\n\nSampling:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 |  ETA: 0:00:09\n\nSampling:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                |  ETA: 0:00:09\n\nSampling:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                |  ETA: 0:00:09\n\nSampling:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                |  ETA: 0:00:08\n\nSampling:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                |  ETA: 0:00:08\n\nSampling:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                |  ETA: 0:00:08\n\nSampling:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               |  ETA: 0:00:08\n\nSampling:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               |  ETA: 0:00:08\n\nSampling:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               |  ETA: 0:00:08\n\nSampling:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               |  ETA: 0:00:08\n\nSampling:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              |  ETA: 0:00:08\n\nSampling:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              |  ETA: 0:00:07\n\nSampling:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              |  ETA: 0:00:07\n\nSampling:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              |  ETA: 0:00:07\n\nSampling:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             |  ETA: 0:00:07\n\nSampling:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             |  ETA: 0:00:07\n\nSampling:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            |  ETA: 0:00:07\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            |  ETA: 0:00:07\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            |  ETA: 0:00:06\n\nSampling:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            |  ETA: 0:00:06\n\nSampling:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           |  ETA: 0:00:06\n\nSampling:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           |  ETA: 0:00:06\n\nSampling:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           |  ETA: 0:00:06\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          |  ETA: 0:00:06\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          |  ETA: 0:00:06\n\nSampling:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          |  ETA: 0:00:05\n\nSampling:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          |  ETA: 0:00:05\n\nSampling:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         |  ETA: 0:00:05\n\nSampling:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         |  ETA: 0:00:05\n\nSampling:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         |  ETA: 0:00:05\n\nSampling:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         |  ETA: 0:00:05\n\nSampling:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         |  ETA: 0:00:05\n\nSampling:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        |  ETA: 0:00:05\n\nSampling:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        |  ETA: 0:00:04\n\nSampling:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        |  ETA: 0:00:04\n\nSampling:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        |  ETA: 0:00:04\n\nSampling:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        |  ETA: 0:00:04\n\nSampling:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       |  ETA: 0:00:04\n\nSampling:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       |  ETA: 0:00:04\n\nSampling:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       |  ETA: 0:00:04\n\nSampling:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       |  ETA: 0:00:04\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      |  ETA: 0:00:04\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      |  ETA: 0:00:03\n\nSampling:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      |  ETA: 0:00:03\n\nSampling:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      |  ETA: 0:00:03\n\nSampling:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     |  ETA: 0:00:03\n\nSampling:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     |  ETA: 0:00:03\n\nSampling:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    |  ETA: 0:00:03\n\nSampling:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    |  ETA: 0:00:02\n\nSampling:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    |  ETA: 0:00:02\n\nSampling:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    |  ETA: 0:00:02\n\nSampling:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    |  ETA: 0:00:02\n\nSampling:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   |  ETA: 0:00:02\n\nSampling:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   |  ETA: 0:00:02\n\nSampling:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   |  ETA: 0:00:02\n\nSampling:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  |  ETA: 0:00:01\n\nSampling:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  |  ETA: 0:00:01\n\nSampling:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  |  ETA: 0:00:01\n\nSampling:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè |  ETA: 0:00:01\n\nSampling:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã |  ETA: 0:00:01\n\nSampling:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà |  ETA: 0:00:01\n\nSampling:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè|  ETA: 0:00:00\n\nSampling:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç|  ETA: 0:00:00\n\nSampling:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä|  ETA: 0:00:00\n\nSampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Time: 0:00:21\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtensions to more complex models is easy. Negative binomial regression\n\\[\ny_{i}\\vert\\boldsymbol{x}_{i} \\sim\\mathrm{NegBinomial}\\left(\\psi,p=\\frac{\\psi}{\\psi+\\lambda_{i}}\\right),\\quad\\lambda_{i}=\\exp(\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta})\n\\]\n# Negative binomial regression \n@model function negbinomialReg(y, X, œÑ, Œº‚ÇÄ, œÉ‚ÇÄ)\n    p = size(X,2)\n    Œ≤ ~ filldist(Normal(0, œÑ), p)  # all Œ≤‚±º are iid Normal(0, œÑ)\n    Œª = exp.(X*Œ≤)\n    œà ~ LogNormal(Œº‚ÇÄ, œÉ‚ÇÄ)             # log of overdispersion parameter\n    n = length(y)  \n    for i in 1:n\n        y[i] ~ NegativeBinomial(œà, œà/(œà + Œª[i])) # mean is Œª here, but var = Œª(1 + Œª/œà) \n    end\nend\n\nŒº‚ÇÄ = 0   # Prior mean of log(œà), where œà is the overdispersion parameter\nœÉ‚ÇÄ = 10  \nŒ± = 0.70  # target acceptance probability in NUTS sampler\nmodel = negbinomialReg(y, X, œÑ, Œº‚ÇÄ, œÉ‚ÇÄ)\nchain = sample(model, NUTS(Œ±), 10000, discard_initial = 1000)\nrateratio = exp.(chain.value)\n\n\n\nVariational inference in Turing.jl\n# Variational inference assuming posterior is independent normals\nnSamples = 10\nnGradSteps = 1000\napprox_post = vi(model, ADVI(nSamples, nGradSteps)) # error when I try it now.\napprox_post.dist.m # mean of variational approximation\napprox_post.dist.œÉ # stdev of variational approximation\nŒ≤sample = rand(approx_post, 10000)"
  },
  {
    "objectID": "interop.html#calling-r-from-julia-code",
    "href": "interop.html#calling-r-from-julia-code",
    "title": "Julia + R + Python = True",
    "section": "",
    "text": "The RCall.jl package makes it possible to call R from Julia.\n\nAn R package or function may be lacking in Julia\nBenchmarking against methods in R packages.\n\nRCall.jl does:\n\nSends R commands to an R session in the background.\nMoves data from Julia to R, and R back to Julia.\nTransforms data structures in Julia to the ‚Äúright‚Äù data structure in R. Dictionary -&gt; list etc.\nEnter R mode by typing $ to get access to R console.\n\nTo send a command to R, use R\"julia_expression\"\n\n\nusing RCall         # Julia session must be set up, see RCall.jl docs\nR\"plot(rnorm(10))\";  # Evaluates in R\n\n\n\n\n\n\n\n\n\nVariables in Julia can be interpolated in R code using the $ -syntax\n\n\nn = 10\nR\"plot(rnorm($n))\";\n\n\n\n\n\n\n\n\n\nJulia variables can be sent to R with @rput .\nR variables can be sent back to Julia with @rget.\n\n\n# Just some regression data to play with\nn = 100\np = 3\nX = randn(n,p)\nŒ≤ = [1, 2, 0.5]\nœÉ = 0.3\ny  = X*Œ≤ + œÉ*randn(n)\n@rput y               # y now lives in the R session\n@rput X\nR\"modelFit &lt;- lm(y ~ X)\"\nR\"betaHat &lt;- modelFit$coef\"\nŒ≤hat = @rget betaHat  # Pull the R variable betaHat back to Julia\nŒ≤hat                  # lives in Julia now\n\n4-element Vector{Float64}:\n 0.043620495361893274\n 0.9772471884035804\n 2.014358371543607\n 0.592342891482621\n\n\n\nFor longer multiline code chunks, use the triple quotes:\n\nz = 2\n@rput z\nR\"\"\"\n    f &lt;- function(x, z) x + z\n    fvalue &lt;- f(1, $z)\n\"\"\"\nout = @rget fvalue # Pull the R variable out back to Julia\n\n3.0\n\n\nWrap R functions into Julia functions (works also for installed packages):\n\nfunction ARMAacf(ar, ma; lagmax, pacf = false) \n    R\"\"\"\n        acf_theo = ARMAacf(ar = $ar, ma = $ma, lag.max = $lagmax, pacf = $pacf)\n    \"\"\"\n    @rget acf_theo\n    return acf_theo\nend\n\nARMAacf([0.5, -0.2], [0.3]; lagmax = 5) # This is Julia function now\n\n6-element Vector{Float64}:\n  1.0\n  0.5646766169154229\n  0.08233830845771144\n -0.07176616915422887\n -0.052350746268656725\n -0.011822139303482589"
  },
  {
    "objectID": "interop.html#calling-julia-from-r",
    "href": "interop.html#calling-julia-from-r",
    "title": "Julia + R + Python = True",
    "section": "Calling Julia from R",
    "text": "Calling Julia from R\n\nThe JuliaCall package in R can be used to call Julia code from R.\nOne time setup (install Julia first):\n\nlibrary(JuliaCall)\noptions(JULIA_HOME = \" ~/.juliaup/bin/julia\")     # Set path to Julia binary\njulia_setup()                                     # Setup Julia\njulia_command(\"a = sqrt(2.0)\")                    # Just testing the install\nHere is a simple example:\njulia_install_package(\"LinearAlgebra\")            # Installs Julia package\njulia_library(\"LinearAlgebra\")                    # Loads Julia package\njulia_install_package(\"Distributions\")           \njulia_library(\"Distributions\")\n\njulia_source(\"code/poisloglik.jl\") # Julia code file, contains:\n#function poisreg_loglik(Œ≤, y, X)                        \n#   return sum(logpdf.(Poisson.(exp.(X*Œ≤)), y))\n#end\n\n# This is the R function definition, wrapping the Julia function\npoisreg_loglik &lt;- function(beta_, y, X){\n  return(julia_call(\"poisreg_loglik\", beta_, y, X))\n}\nX = cbind(1, matrix(rnorm(100)))\nbeta_ = c(0.5,-0.5)\ny = rpois(100, lambda = exp(X %*% beta_))\npoisreg_loglik(beta_, y, X)\n\nThe R package https://github.com/Non-Contradiction/autodiffr uses JuliaCall to bring autodiff to R."
  },
  {
    "objectID": "interop.html#using-python-from-julia",
    "href": "interop.html#using-python-from-julia",
    "title": "Julia + R + Python = True",
    "section": "Using Python from Julia",
    "text": "Using Python from Julia\n\nThe PyCall.jl package makes it possible to use Python code from Julia.\nAt setup, you may need to tell PyCall.jl where your Python install is with ENV[\"PYTHON\"] = \"~/anaconda3/bin/python\" (this is my path) and then re-build PyCall from the package manager with ] build PyCall\nHere is an example using the matplotlib library in Python for plotting\n\nusing PyCall\nx = range(0; stop=2*pi, length=1000)\ny = sin.(3*x)\nplt = pyimport(\"matplotlib.pyplot\")  # importing matplotlib as plt\nplt.plot(x, y, color=\"red\", linewidth=2.0, linestyle=\"--\")\nplt.show()\n\nFigure(640x480)\n\n\nUsing Newton‚Äôs method in the Python package SciPy for root finding\n\nso = pyimport(\"scipy.optimize\")\nso.newton(x -&gt; cos(x) - x, 1)\n\n0.7390851332151607\n\n\nUsing Python function in Julia:\n\npy\"\"\"\nimport numpy as np\n\ndef my_sin(x):\n    print(\"hello from the Python side.\")\n    return np.sin(x)\n\"\"\"\npy\"my_sin\"(œÄ/2)\n\nhello from the Python side.\n\n\n1.0\n\n\nWrap a Python function in a Julia function\n\n\nfunction my_sin(x)\n    return(py\"my_sin\"(x))\nend\nmy_sin(œÄ/2) # This is a Julia function now\n\nhello from the Python side.\n\n\n1.0"
  },
  {
    "objectID": "packagemanager.html",
    "href": "packagemanager.html",
    "title": "The package manager",
    "section": "",
    "text": "More info will be added here later."
  },
  {
    "objectID": "CFECM2025.html",
    "href": "CFECM2025.html",
    "title": "Probabilistic programming for statistical analysis in Julia",
    "section": "",
    "text": "This tutorial is part of the COST action HiTEc, prepared for the CFE-CMStatistics 2025 conference in London.\n\nJulia has emerged as an important language for statistical data analysis and machine learning. It is a high-level language that is easy to learn, but with a speed close to C/C++ from its just-in-time compilation. Despite its relatively young age, Julia already has an impressive set of libraries for statistics, and can be easily integrated with a workflow in R or Python.\nThis first half of this tutorial introduces the Julia programming language with a focus on statistical analysis. The second half focuses on likelihood and Bayesian inference using the Turing.jl probabilistic programming ecosystem in Julia. Participants are encouraged to install Julia and some statistical packages before the tutorial, to follow along on their own laptops.\n\nIntroduction to the Julia programming language\nInstructor: Mattias Villani, Professor of Statistics, Stockholm University.\nWeb site for the material: Julia4Stats.\n\n\nProbabilistic programming for statistical analysis in Turing.jl\nInstructors:\n\nMarkus Hauru, Senior Research Data Scientist, Alan Turing Institute\nPenelope Yong, Research Data Scientist, Alan Turing Institute\nTim Hargreaves, PhD candidate, University of Cambridge"
  }
]